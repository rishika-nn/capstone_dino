{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unified search helper: optional upload flow + smart search with thumbnails\n",
        "import os, time\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Assume engine is already initialized in prior cells\n",
        "try:\n",
        "    engine\n",
        "except NameError:\n",
        "    from video_search_engine import VideoSearchEngine\n",
        "    engine = VideoSearchEngine()\n",
        "\n",
        "# Helper to process a video if provided; else skip\n",
        "\n",
        "def process_video_if_any(video_path: str = None, video_name: str = None):\n",
        "    if video_path and os.path.exists(video_path):\n",
        "        name = video_name or os.path.splitext(os.path.basename(video_path))[0]\n",
        "        print(f\"\\nüé¨ Processing video: {name}\")\n",
        "        stats = engine.process_video(\n",
        "            video_path=video_path,\n",
        "            video_name=name,\n",
        "            save_frames=False,\n",
        "            upload_to_pinecone=True,\n",
        "            use_object_detection=False\n",
        "        )\n",
        "        print(\"‚úÖ Processing complete.\")\n",
        "        return name, stats\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No video uploaded. Searching existing Pinecone index.\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def search_and_display(query: str, video_filter: str = None, top_k: int = 5):\n",
        "    print(f\"\\nüîé Query: {query}\")\n",
        "    results = engine.smart_search(\n",
        "        query=query,\n",
        "        top_k=top_k,\n",
        "        video_filter=video_filter\n",
        "    )\n",
        "    if not results:\n",
        "        print(\"No results found.\")\n",
        "        return\n",
        "\n",
        "    for i, r in enumerate(results, 1):\n",
        "        ts = r.get('timestamp', 0.0)\n",
        "        print(f\"{i}. ‚è± {r.get('time_formatted','')}  üìä {r.get('similarity_score',0):.0%}\")\n",
        "        print(f\"   üìù {r.get('caption','')}\")\n",
        "        print(f\"   üé• {r.get('video_name','')}\")\n",
        "        thumb_path = r.get('thumbnail_path')\n",
        "        if thumb_path and os.path.exists(thumb_path):\n",
        "            try:\n",
        "                img = Image.open(thumb_path)\n",
        "                display(img)\n",
        "            except Exception:\n",
        "                pass\n",
        "        elif r.get('video_name') in getattr(engine, 'video_paths', {}):\n",
        "            # Fallback: extract and show directly\n",
        "            vpath = engine.video_paths[r['video_name']]\n",
        "            cap = cv2.VideoCapture(vpath)\n",
        "            if cap.isOpened():\n",
        "                fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "                frame_num = int(max(0, ts) * fps)\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "                ok, frame = cap.read()\n",
        "                cap.release()\n",
        "                if ok and frame is not None:\n",
        "                    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    display(Image.fromarray(rgb))\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pinecone setup: dual indexes (text + image)\n",
        "Ensure two Pinecone indexes exist and match your config:\n",
        "- Text index: dimension must match your text embedding model (default 1024)\n",
        "- Image index: dimension must match CLIP image/text space (default 512)\n",
        "\n",
        "Run the cell below to verify/create the indexes automatically using values from `video_search_config.Config`. You need `PINECONE_API_KEY` set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify or create Pinecone text/image indexes per config\n",
        "import os, time\n",
        "from video_search_config import Config\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "assert Config.PINECONE_API_KEY, \"PINECONE_API_KEY is required\"\n",
        "\n",
        "pc = Pinecone(api_key=Config.PINECONE_API_KEY)\n",
        "existing = {idx.name for idx in pc.list_indexes()}\n",
        "\n",
        "needed = [\n",
        "    (Config.PINECONE_TEXT_INDEX_NAME, Config.PINECONE_TEXT_DIMENSION),\n",
        "    (Config.PINECONE_IMAGE_INDEX_NAME, Config.PINECONE_IMAGE_DIMENSION),\n",
        "]\n",
        "\n",
        "for name, dim in needed:\n",
        "    if name in existing:\n",
        "        print(f\"‚úÖ Index exists: {name}\")\n",
        "        # sanity check dimension (best-effort)\n",
        "        try:\n",
        "            idx = pc.Index(name)\n",
        "            stats = idx.describe_index_stats()\n",
        "            actual_dim = stats.get('dimension')\n",
        "            if actual_dim and int(actual_dim) != int(dim):\n",
        "                print(f\"‚ö†Ô∏è Dimension mismatch for {name}: index={actual_dim} config={dim}\")\n",
        "            else:\n",
        "                print(f\"   Dimension OK: {dim}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   (warn) Could not verify dimension for {name}: {e}\")\n",
        "    else:\n",
        "        print(f\"üì¶ Creating index: {name} (dim={dim})\")\n",
        "        pc.create_index(\n",
        "            name=name,\n",
        "            dimension=int(dim),\n",
        "            metric=Config.PINECONE_METRIC,\n",
        "            spec=ServerlessSpec(cloud=Config.PINECONE_CLOUD, region=Config.PINECONE_REGION)\n",
        "        )\n",
        "        # brief wait\n",
        "        time.sleep(5)\n",
        "        print(f\"   ‚úÖ Created: {name}\")\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Video Frame Search System with InstructBLIP & Pinecone\n",
        "\n",
        "This notebook sets up a complete video semantic search engine that:\n",
        "- Extracts frames from videos\n",
        "- Generates object-focused captions using InstructBLIP (better than BLIP for object attributes)\n",
        "- Stores embeddings in Pinecone\n",
        "- Enables natural language search with temporal bootstrapping\n",
        "\n",
        "**Features:**\n",
        "- üéØ Object-focused captioning (detects colors, attributes like \"black backpack\", \"red shirt\")\n",
        "- üöÄ Temporal bootstrapping (finds related objects at similar timestamps)\n",
        "- üìä Adaptive search windows (adjusts based on motion)\n",
        "- ‚ö° Confidence-aware boosting\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        " ## Step 1: Setup - Clone Repository & Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup",
        "outputId": "2f0a2bcf-207f-4119-99b3-68e7bd0f2b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'capstone-BLIP'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 69 (delta 32), reused 55 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (69/69), 100.55 KiB | 5.03 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n",
            "/content/capstone-BLIP/capstone-BLIP/capstone-BLIP/capstone-BLIP/capstone-BLIP/capstone-BLIP/capstone-BLIP/capstone-BLIP/capstone-BLIP\n",
            "üì¶ Installing dependencies... This will take 3-5 minutes\n",
            "\n",
            "‚úÖ Installation complete!\n",
            "\n",
            "üöÄ GPU detected: Tesla T4\n",
            "   Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/pranavacchu/capstone-BLIP.git\n",
        "%cd capstone-BLIP\n",
        "\n",
        "# Install dependencies\n",
        "print(\"üì¶ Installing dependencies... This will take 3-5 minutes\")\n",
        "%pip install -q opencv-python-headless pillow numpy pandas tqdm python-dotenv\n",
        "%pip install -q torch torchvision transformers sentence-transformers\n",
        "%pip install -q pinecone FlagEmbedding\n",
        "print(\"\\n‚úÖ Installation complete!\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nüöÄ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No GPU detected. Using CPU (slower but works)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRq_cGCWMbER",
        "outputId": "7ff005a6-b18b-4f91-f8c2-35e36bc1fcd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Applying hotfixes...\n",
            "   - Adding deduplicate_embeddings method...\n",
            "   ‚úì deduplicate_embeddings already exists\n",
            "   - Fixing Grounding DINO dtype mismatch...\n",
            "   ‚ö† Could not find code to replace in object_detector.py\n",
            "\n",
            "‚úÖ Hotfixes applied successfully!\n",
            "   You can now proceed with video processing\n"
          ]
        }
      ],
      "source": [
        "# Verify imports work correctly\n",
        "try:\n",
        "    from video_search_engine import VideoSearchEngine\n",
        "    print(\"‚úÖ All modules loaded successfully!\")\n",
        "    print(\"üìù Using InstructBLIP model for object-focused captioning\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
        "    print(\"   Please check that all files are in the correct directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZoe_yvwNc5Y",
        "outputId": "8fa2987e-ca93-4187-b747-89bd8d13557e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Reloaded\n"
          ]
        }
      ],
      "source": [
        "# Cell removed - all imports verified in cell 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeN8iRp-vG6a",
        "outputId": "95c7fde9-9d35-4fe0-d2d4-3444df8d580a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing Grounding DINO and dependencies...\n",
            "This may take 2-3 minutes...\n",
            "\n",
            "Grounding DINO dependencies installed!\n",
            "Models will be downloaded automatically from Hugging Face on first use\n"
          ]
        }
      ],
      "source": [
        "# Cell removed - all dependencies installed in cell 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Configure Pinecone API Key\n",
        "\n",
        "The system will load credentials from a `.env` file. If the file doesn't exist, you'll be prompted to create it.\n",
        "\n",
        "**Required settings:**\n",
        "- **API Key**: Your Pinecone API key\n",
        "- **Index Host**: Your index URL (from Pinecone dashboard)\n",
        "- **Environment**: Usually `us-east-1` or your Pinecone region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config",
        "outputId": "f66f962e-f2fa-4241-ea95-64a80f2872fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration saved!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Check if .env file exists\n",
        "env_exists = os.path.exists('.env')\n",
        "\n",
        "if not env_exists:\n",
        "    print(\"üìù No .env file found. Creating one...\")\n",
        "    print(\"\\nPlease enter your Pinecone credentials:\")\n",
        "    \n",
        "    # Get credentials from user\n",
        "    api_key = input(\"Enter your Pinecone API Key: \").strip()\n",
        "    host = input(\"Enter your Pinecone Host URL (e.g., https://xxx.svc.xxx.pinecone.io): \").strip()\n",
        "    environment = input(\"Enter your Pinecone Environment (e.g., us-east-1, default=us-east-1): \").strip() or \"us-east-1\"\n",
        "    \n",
        "    # Write to .env file\n",
        "    with open('.env', 'w') as f:\n",
        "        f.write(f\"PINECONE_API_KEY={api_key}\\n\")\n",
        "        f.write(f\"PINECONE_HOST={host}\\n\")\n",
        "        f.write(f\"PINECONE_ENVIRONMENT={environment}\\n\")\n",
        "    \n",
        "    print(\"\\n‚úÖ .env file created!\")\n",
        "else:\n",
        "    print(\"‚úÖ .env file found. Loading credentials...\")\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify credentials are loaded\n",
        "api_key = os.getenv('PINECONE_API_KEY')\n",
        "host = os.getenv('PINECONE_HOST')\n",
        "\n",
        "if api_key and host:\n",
        "    print(\"‚úÖ Credentials loaded successfully!\")\n",
        "    print(f\"   API Key: {api_key[:20]}...{api_key[-10:] if len(api_key) > 30 else ''}\")\n",
        "    print(f\"   Host: {host}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Warning: Some credentials are missing from .env file\")\n",
        "    print(\"   Please check your .env file and ensure it contains:\")\n",
        "    print(\"   - PINECONE_API_KEY\")\n",
        "    print(\"   - PINECONE_HOST\")\n",
        "    print(\"   - PINECONE_ENVIRONMENT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "##  Step 3: Test Connection to Pinecone\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_connection",
        "outputId": "e5504b78-9687-482a-9bf9-ae7a45663f33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîå Connecting to Pinecone...\n",
            "\n",
            "‚úÖ Successfully connected to Pinecone!\n",
            "\n",
            "üìä Database Statistics:\n",
            "   Index: capstone\n",
            "   Total vectors: 35\n",
            "   Dimension: 1024\n",
            "   Capacity: Serverless\n"
          ]
        }
      ],
      "source": [
        "from video_search_engine import VideoSearchEngine\n",
        "\n",
        "print(\"üîå Connecting to Pinecone...\")\n",
        "engine = VideoSearchEngine()\n",
        "\n",
        "# Get database stats\n",
        "stats = engine.get_index_stats()\n",
        "\n",
        "print(\"\\n‚úÖ Successfully connected to Pinecone!\")\n",
        "print(f\"\\nüìä Database Statistics:\")\n",
        "print(f\"   Index: capstone\")\n",
        "print(f\"   Total vectors: {stats.get('total_vectors', 0):,}\")\n",
        "print(f\"   Dimension: {stats.get('dimension', 1024)}\")\n",
        "print(f\"   Capacity: Serverless\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Upload a Video File\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upload_video",
        "outputId": "708647a2-4a4d-4096-8368-d49d70e3d6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì§ Choose how to get your video:\n",
            "\n",
            "1. Upload from computer (recommended for small files < 100MB)\n",
            "2. Download from URL (direct video file)\n",
            "3. Download from YouTube URL\n",
            "\n",
            "Enter choice (1/2/3): 3\n",
            "\n",
            "Enter YouTube URL (video or shorts): https://www.youtube.com/shorts/QhlroYnundk\n",
            "‚¨áÔ∏è Downloading from YouTube...\n",
            "   Installing yt-dlp (if needed)...\n",
            "   Fetching video info...\n",
            "‚úÖ Downloaded successfully: youtube_video.mp4\n",
            "\n",
            "üìπ Video ready: youtube_video.mp4 (2.2 MB)\n",
            "   Duration: 7.6 seconds\n",
            "   FPS: 30.0\n",
            "   Total frames: 228\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import subprocess\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "print(\"üì§ Choose how to get your video:\\n\")\n",
        "print(\"1. Upload from computer (recommended for small files < 100MB)\")\n",
        "print(\"2. Download from URL (direct video file)\")\n",
        "print(\"3. Download from YouTube URL\\n\")\n",
        "\n",
        "choice = input(\"Enter choice (1/2/3): \").strip()\n",
        "video_path = None\n",
        "\n",
        "if choice == \"1\":\n",
        "    print(\"\\nüìÅ Please select your video file...\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        video_path = list(uploaded.keys())[0]\n",
        "        print(f\"‚úÖ Uploaded: {video_path}\")\n",
        "    else:\n",
        "        print(\"‚ùå No file uploaded\")\n",
        "\n",
        "elif choice == \"2\":\n",
        "    video_url = input(\"\\nEnter video URL (direct link to .mp4, .avi, etc.): \").strip()\n",
        "\n",
        "    if not video_url:\n",
        "        print(\"‚ùå No URL provided\")\n",
        "    else:\n",
        "        # Extract filename from URL or use default\n",
        "        parsed_url = urlparse(video_url)\n",
        "        url_filename = os.path.basename(parsed_url.path)\n",
        "\n",
        "        # Use URL filename if it has an extension, otherwise use default\n",
        "        if url_filename and '.' in url_filename:\n",
        "            video_filename = url_filename\n",
        "        else:\n",
        "            video_filename = \"downloaded_video.mp4\"\n",
        "\n",
        "        print(f\"‚¨áÔ∏è Downloading from URL...\")\n",
        "        print(f\"   Target file: {video_filename}\")\n",
        "\n",
        "        try:\n",
        "            # Use subprocess for better control\n",
        "            result = subprocess.run(\n",
        "                ['wget', '-O', video_filename, video_url, '--no-check-certificate', '-q', '--show-progress'],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=300\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0 and os.path.exists(video_filename):\n",
        "                if os.path.getsize(video_filename) > 0:\n",
        "                    video_path = video_filename\n",
        "                    print(f\"‚úÖ Downloaded successfully: {video_filename}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Download failed: File is empty\")\n",
        "                    if os.path.exists(video_filename):\n",
        "                        os.remove(video_filename)\n",
        "            else:\n",
        "                print(f\"‚ùå Download failed: wget returned code {result.returncode}\")\n",
        "                # Try alternative method with curl\n",
        "                print(\"\\nüîÑ Trying alternative download method (curl)...\")\n",
        "                result2 = subprocess.run(\n",
        "                    ['curl', '-L', '-o', video_filename, video_url, '--silent', '--show-error'],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=300\n",
        "                )\n",
        "\n",
        "                if result2.returncode == 0 and os.path.exists(video_filename) and os.path.getsize(video_filename) > 0:\n",
        "                    video_path = video_filename\n",
        "                    print(f\"‚úÖ Downloaded successfully with curl: {video_filename}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Alternative download also failed\")\n",
        "                    print(\"   Please check if the URL is accessible and try again\")\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Download timed out (>5 minutes). File may be too large.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Download error: {e}\")\n",
        "\n",
        "elif choice == \"3\":\n",
        "    youtube_url = input(\"\\nEnter YouTube URL (video or shorts): \").strip()\n",
        "\n",
        "    if not youtube_url:\n",
        "        print(\"‚ùå No URL provided\")\n",
        "    else:\n",
        "        print(\"‚¨áÔ∏è Downloading from YouTube...\")\n",
        "        print(\"   Installing yt-dlp (if needed)...\")\n",
        "\n",
        "        # Install yt-dlp if not present\n",
        "        subprocess.run(['pip', 'install', '-q', 'yt-dlp'], check=False)\n",
        "\n",
        "        video_filename = \"youtube_video.mp4\"\n",
        "\n",
        "        try:\n",
        "            print(f\"   Fetching video info...\")\n",
        "\n",
        "            # Download with yt-dlp\n",
        "            result = subprocess.run(\n",
        "                [\n",
        "                    'yt-dlp',\n",
        "                    '-f', 'best[ext=mp4]/best',  # Best quality MP4\n",
        "                    '-o', video_filename,\n",
        "                    '--no-playlist',\n",
        "                    '--quiet',\n",
        "                    '--progress',\n",
        "                    youtube_url\n",
        "                ],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=600  # 10 minute timeout for YouTube\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0 and os.path.exists(video_filename):\n",
        "                if os.path.getsize(video_filename) > 0:\n",
        "                    video_path = video_filename\n",
        "                    print(f\"‚úÖ Downloaded successfully: {video_filename}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Download failed: File is empty\")\n",
        "                    if os.path.exists(video_filename):\n",
        "                        os.remove(video_filename)\n",
        "            else:\n",
        "                print(f\"‚ùå YouTube download failed\")\n",
        "                if result.stderr:\n",
        "                    print(f\"   Error: {result.stderr[:300]}\")\n",
        "                print(\"\\nüí° Troubleshooting tips:\")\n",
        "                print(\"   - Make sure the video is public and not age-restricted\")\n",
        "                print(\"   - Try using Option 1 to upload the video manually\")\n",
        "                print(\"   - Check if the URL is correct\")\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚ùå Download timed out (>10 minutes).\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Download error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Invalid choice. Please choose option 1, 2, or 3.\")\n",
        "\n",
        "# Validate the video file\n",
        "if video_path:\n",
        "    if os.path.exists(video_path):\n",
        "        file_size = os.path.getsize(video_path) / (1024*1024)  # MB\n",
        "        print(f\"\\nüìπ Video ready: {video_path} ({file_size:.1f} MB)\")\n",
        "\n",
        "        # Verify it's a valid video file\n",
        "        import cv2\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if cap.isOpened():\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            duration = frame_count / fps if fps > 0 else 0\n",
        "            print(f\"   Duration: {duration:.1f} seconds\")\n",
        "            print(f\"   FPS: {fps:.1f}\")\n",
        "            print(f\"   Total frames: {frame_count:,}\")\n",
        "            cap.release()\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è Warning: Unable to read video file. It may be corrupted.\")\n",
        "            print(\"   Please try a different video or URL.\")\n",
        "            video_path = None\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Error: File not found at {video_path}\")\n",
        "        video_path = None\n",
        "\n",
        "if not video_path:\n",
        "    print(\"\\n‚ùå No valid video file available. Please run this cell again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwRP7HFQvYl1",
        "outputId": "99f1ea5f-f374-4f9a-e54a-fe9e13bc205a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose your captioning method:\n",
            "\n",
            "1. Standard BLIP (faster, general scene captions)\n",
            "2. Object Detection + BLIP (slower, object-focused)\n",
            "\n",
            "Enter choice (1/2, default=1): 2\n",
            "\n",
            "Using Object Detection + BLIP pipeline\n",
            "   Detects objects: bags, laptops, helmets, phones, etc.\n"
          ]
        }
      ],
      "source": [
        "# Using InstructBLIP for object-focused captioning\n",
        "print(\"üìù Using InstructBLIP for captioning\")\n",
        "print(\"   Focuses on object attributes (colors, sizes, etc.)\")\n",
        "print(\"   Better than standard BLIP for object descriptions like 'black backpack', 'red shirt'\")\n",
        "use_object_detection = False  # Object detection mode disabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Process the Video\n",
        "\n",
        "This will:\n",
        "1. Extract frames from the video (removing redundant frames)\n",
        "2. Generate captions using **InstructBLIP** (object-focused with colors and attributes)\n",
        "3. Create embeddings for semantic search\n",
        "4. Upload to Pinecone database\n",
        "\n",
        "**Expected time:**\n",
        "- 1 minute video: ~2-3 minutes with GPU\n",
        "- 5 minute video: ~8-10 minutes with GPU\n",
        "- CPU mode: 3-5x slower\n",
        "\n",
        "**Caption Quality:**\n",
        "- InstructBLIP provides detailed object descriptions with colors and attributes\n",
        "- Example: \"a person wearing a red shirt and blue jeans with a black backpack\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba691f1082b9431788be4e50e8141b3e",
            "c50b623424624bcabf621afcd01a7e1e",
            "62e897ad4342444da1192278dfee423e",
            "9518db97018c4e9b8f600c1c505b8e50",
            "842f35e1b1204c93878d78267040688e",
            "23620e7e22174d64a296a93987c17fec",
            "41eb84dbfbaa4f44b86bc3c19e7b5ae1",
            "ef51046f4b9d4f69acbb93c2c06782d6",
            "f0c45d6c2582411da54de81095263e6c",
            "0b8e85f723ff498cb32ecabbc9e8d7e6",
            "2ecf964c16ff4a08878fe34558a95210"
          ]
        },
        "id": "process_video",
        "outputId": "bdfa4ed4-e239-4b4d-938b-c582dc411deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a name for this video (or press Enter for auto-name): dino\n",
            "\n",
            "üé¨ Processing video: dino\n",
            "‚è≥ This will take a few minutes... Please wait.\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 228/228 [00:01<00:00, 137.99it/s]\n",
            "Processing frames:   0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/models/grounding_dino/processing_grounding_dino.py:93: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
            "  warnings.warn(self.message, FutureWarning)\n",
            "ERROR:object_caption_pipeline:Error captioning object backpack: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object car: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object school bag laptop bag: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "Processing frames:  20%|‚ñà‚ñà        | 1/5 [00:03<00:13,  3.26s/it]ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object backpack: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object school bag laptop bag: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "Processing frames:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:05<00:08,  2.76s/it]ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object backpack: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object school bag laptop bag: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "Processing frames:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:07<00:04,  2.50s/it]ERROR:object_caption_pipeline:Error captioning object backpack: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object school bag laptop bag: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "Processing frames:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:10<00:02,  2.48s/it]ERROR:object_caption_pipeline:Error captioning object backpack: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object person student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object car: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object student: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "ERROR:object_caption_pipeline:Error captioning object school bag laptop bag: 'ObjectCaptionPipeline' object has no attribute '_score_attribute_caption'\n",
            "Processing frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:12<00:00,  2.57s/it]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba691f1082b9431788be4e50e8141b3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Uploading to Pinecone: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "\n",
            "‚úÖ VIDEO PROCESSING COMPLETE!\n",
            "\n",
            "üìä Processing Statistics:\n",
            "   Video name: dino\n",
            "   Frames extracted: 5\n",
            "   Frames with captions: 2\n",
            "   Captions before dedupe: 2\n",
            "   Unique embeddings: 1\n",
            "   ‚úÖ Actually uploaded: 1\n",
            "   Processing time: 0.4 minutes\n",
            "\n",
            "   Frame reduction: 60.0%\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "if 'video_path' not in locals() or not video_path:\n",
        "    print(\"‚ùå Please upload a video first (run the previous cell)\")\n",
        "else:\n",
        "    # Set video name\n",
        "    video_name = input(\"Enter a name for this video (or press Enter for auto-name): \").strip()\n",
        "    if not video_name:\n",
        "        video_name = f\"video_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "    print(f\"\\nüé¨ Processing video: {video_name}\")\n",
        "    print(\"‚è≥ This will take a few minutes... Please wait.\\n\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Process the video with InstructBLIP\n",
        "        stats = engine.process_video(\n",
        "            video_path=video_path,\n",
        "            video_name=video_name,\n",
        "            save_frames=False,  # Set to True to save frames\n",
        "            upload_to_pinecone=True,\n",
        "            use_object_detection=False  # Using InstructBLIP only (no Grounding DINO)\n",
        "        )\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"\\n‚úÖ VIDEO PROCESSING COMPLETE!\\n\")\n",
        "        print(f\"üìä Processing Statistics:\")\n",
        "        print(f\"   Video name: {video_name}\")\n",
        "        print(f\"   Frames extracted: {stats['total_frames_extracted']:,}\")\n",
        "        print(f\"   Frames with captions: {stats['frames_with_captions']:,}\")\n",
        "        print(f\"   Captions before dedupe: {stats.get('captions_before_dedupe', stats['frames_with_captions']):,}\")\n",
        "        print(f\"   Unique embeddings: {stats.get('embeddings_generated', 0):,}\")\n",
        "        print(f\"   ‚úÖ Actually uploaded: {stats['embeddings_uploaded']:,}\")\n",
        "        print(f\"   Processing time: {processing_time/60:.1f} minutes\")\n",
        "        print(f\"\\n   Frame reduction: {stats.get('frame_reduction_percent', 0):.1f}%\")\n",
        "\n",
        "        # Save video_name for next steps\n",
        "        processed_video_name = video_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error processing video: {e}\")\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"- If GPU memory error: Restart runtime and try again\")\n",
        "        print(\"- If video format error: Convert video to MP4 format\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: Search Your Video!\n",
        "\n",
        "Search using natural language queries. The system uses **temporal bootstrapping** to find related objects automatically!\n",
        "\n",
        "**Features in action:**\n",
        "- üîÑ **Temporal Bootstrapping**: Finds related objects at similar timestamps\n",
        "- üìä **Adaptive Window**: Adjusts search window based on motion intensity\n",
        "- ‚ö° **Confidence-Aware**: Weights results by detection confidence\n",
        "\n",
        "**Example queries:**\n",
        "- \"person walking\" ‚Üí Also finds \"bag\", \"backpack\" nearby\n",
        "- \"red shirt\" ‚Üí Also finds \"blue bag\", \"person\" at similar times\n",
        "- \"black bag\" ‚Üí Also finds related objects in the same scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "search_single"
      },
      "outputs": [],
      "source": [
        "# Search with temporal bootstrapping (uses all 3 novel features!)\n",
        "query = input(\"üîç Enter your search query: \").strip()\n",
        "\n",
        "print(f\"\\nüîç Searching for: '{query}' with temporal bootstrapping...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚ú® Using: Temporal Bootstrapping + Adaptive Window + Confidence-Aware Boosting\\n\")\n",
        "\n",
        "# Use search_with_bootstrapping for intelligent search\n",
        "results_dict = engine.search_with_bootstrapping(\n",
        "    primary_query=query,\n",
        "    auto_extract_related=True,  # Automatically find related objects\n",
        "    top_k=5,\n",
        "    similarity_threshold=0.5,\n",
        "    video_filter=processed_video_name if 'processed_video_name' in locals() else None\n",
        ")\n",
        "\n",
        "# Display results\n",
        "if results_dict and query in results_dict:\n",
        "    primary_results = results_dict[query]\n",
        "    related_queries = [q for q in results_dict.keys() if q != query]\n",
        "    \n",
        "    print(f\"‚úÖ Primary query '{query}' found {len(primary_results)} results:\\n\")\n",
        "    for i, result in enumerate(primary_results, 1):\n",
        "        print(f\"{i}. ‚è±Ô∏è Timestamp: {result['time_formatted']}\")\n",
        "        print(f\"   üìù Caption: {result['caption']}\")\n",
        "        print(f\"   üìä Confidence: {result['similarity_score']:.1%}\")\n",
        "        print(f\"   üé• Video: {result['video_name']}\")\n",
        "        print()\n",
        "    \n",
        "    # Show boosted related results\n",
        "    if related_queries:\n",
        "        print(f\"\\nüîÑ Temporal Bootstrapping also found related objects:\\n\")\n",
        "        for related_q in related_queries[:3]:  # Show top 3 related queries\n",
        "            related_results = results_dict.get(related_q, [])\n",
        "            if related_results:\n",
        "                print(f\"   üìå Related: '{related_q}' ({len(related_results)} boosted results)\")\n",
        "                for result in related_results[:2]:  # Show top 2\n",
        "                    boost_info = f\" (boosted from {result.get('original_score', 0):.1%})\" if 'original_score' in result else \"\"\n",
        "                    print(f\"      ‚îî‚îÄ {result['time_formatted']}: {result['caption'][:50]}... ({result['similarity_score']:.1%}{boost_info})\")\n",
        "                print()\n",
        "else:\n",
        "    print(\"\\n‚ùå No results found. Try:\")\n",
        "    print(\"   - Different search terms\")\n",
        "    print(\"   - More general queries\")\n",
        "    print(\"   - Lowering the similarity threshold\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: Batch Search (Multiple Queries)\n",
        "\n",
        "Search for multiple things at once! Each query uses temporal bootstrapping automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_search"
      },
      "outputs": [],
      "source": [
        "# Define multiple queries\n",
        "queries = [\n",
        "    \"person walking\",\n",
        "    \"black bag\",\n",
        "    \"red shirt\",\n",
        "    \"outdoor scene\"\n",
        "]\n",
        "\n",
        "print(\"üîç Running batch search with temporal bootstrapping...\\n\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚ú® Each query uses: Temporal Bootstrapping + Adaptive Windows + Confidence-Aware\\n\")\n",
        "\n",
        "batch_results = {}\n",
        "for query in queries:\n",
        "    # Use temporal bootstrapping for each query\n",
        "    results_dict = engine.search_with_bootstrapping(\n",
        "        primary_query=query,\n",
        "        auto_extract_related=True,\n",
        "        top_k=3\n",
        "    )\n",
        "    \n",
        "    if results_dict and query in results_dict:\n",
        "        batch_results[query] = results_dict[query]\n",
        "\n",
        "# Display results\n",
        "for query, results in batch_results.items():\n",
        "    print(f\"\\nüìå Query: '{query}'\")\n",
        "    print(f\"   Found {len(results)} results\")\n",
        "\n",
        "    if results:\n",
        "        for result in results[:2]:  # Show top 2\n",
        "            print(f\"   ‚îî‚îÄ {result['time_formatted']} - {result['caption'][:50]}... ({result['similarity_score']:.0%})\")\n",
        "    else:\n",
        "        print(\"   ‚îî‚îÄ No results\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\\n‚ú® Note: Temporal bootstrapping automatically found related objects for each query!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Advanced Search (Filter by Video)\n",
        "\n",
        "Search with video filter - still uses temporal bootstrapping for intelligent results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced_search"
      },
      "outputs": [],
      "source": [
        "# Advanced search with temporal bootstrapping (filter by video)\n",
        "query = input(\"üîç Enter search query: \").strip()\n",
        "\n",
        "# Optional: Filter by video name\n",
        "video_filter = None\n",
        "if 'processed_video_name' in locals():\n",
        "    filter_video = input(f\"\\nSearch only in '{processed_video_name}'? (y/n): \").lower() == 'y'\n",
        "    if filter_video:\n",
        "        video_filter = processed_video_name\n",
        "\n",
        "# Perform search with temporal bootstrapping\n",
        "print(f\"\\nüîç Searching with temporal bootstrapping...\")\n",
        "print(\"‚ú® Features: Adaptive windows + Confidence-aware boosting\\n\")\n",
        "\n",
        "results_dict = engine.search_with_bootstrapping(\n",
        "    primary_query=query,\n",
        "    auto_extract_related=True,\n",
        "    top_k=10,\n",
        "    similarity_threshold=0.4,  # Lower threshold for more results\n",
        "    video_filter=video_filter\n",
        ")\n",
        "\n",
        "# Display results\n",
        "if results_dict and query in results_dict:\n",
        "    primary_results = results_dict[query]\n",
        "    print(f\"\\n‚úÖ Found {len(primary_results)} results for '{query}':\\n\")\n",
        "    for i, result in enumerate(primary_results, 1):\n",
        "        print(f\"{i}. {result['time_formatted']} - {result['caption'][:60]}... ({result['similarity_score']:.1%})\")\n",
        "    \n",
        "    # Show related objects found\n",
        "    related_queries = [q for q in results_dict.keys() if q != query]\n",
        "    if related_queries:\n",
        "        print(f\"\\nüîÑ Also found {len(related_queries)} related object types at similar timestamps!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No results found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step11"
      },
      "source": [
        "## Step 9: Interactive Search Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive"
      },
      "outputs": [],
      "source": [
        "print(\"üéØ INTERACTIVE VIDEO SEARCH (with Temporal Bootstrapping)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚ú® Features active: Temporal Bootstrapping + Adaptive Windows + Confidence-Aware\")\n",
        "print(\"Enter your search queries (type 'quit' to exit)\\n\")\n",
        "\n",
        "import cv2\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def display_frame_image(video_name: str, timestamp_sec: float):\n",
        "    \"\"\"Extract and display a frame image from the video at the given timestamp.\"\"\"\n",
        "    # Try to resolve the exact video path from the engine (preferred), else fall back to last uploaded path\n",
        "    vpath = None\n",
        "    try:\n",
        "        if hasattr(engine, 'video_paths') and video_name in engine.video_paths:\n",
        "            vpath = engine.video_paths.get(video_name)\n",
        "    except Exception:\n",
        "        vpath = None\n",
        "    if not vpath and 'video_path' in globals():\n",
        "        vpath = video_path\n",
        "    if not vpath:\n",
        "        print(\"   ‚ö†Ô∏è Unable to resolve video path to display frame.\")\n",
        "        return\n",
        "    \n",
        "    cap = cv2.VideoCapture(vpath)\n",
        "    if not cap.isOpened():\n",
        "        print(\"   ‚ö†Ô∏è Unable to open video to extract frame.\")\n",
        "        return\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    frame_num = int(max(0, timestamp_sec) * fps)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "    ok, frame = cap.read()\n",
        "    cap.release()\n",
        "    if not ok or frame is None:\n",
        "        print(\"   ‚ö†Ô∏è Could not read frame at timestamp.\")\n",
        "        return\n",
        "    # Convert BGR -> RGB and display\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    img = Image.fromarray(rgb)\n",
        "    display(img)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nüîç Search: \").strip()\n",
        "\n",
        "    if query.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"\\nüëã Goodbye!\")\n",
        "        break\n",
        "\n",
        "    if not query:\n",
        "        continue\n",
        "\n",
        "    # Use temporal bootstrapping search\n",
        "    results_dict = engine.search_with_bootstrapping(\n",
        "        primary_query=query,\n",
        "        auto_extract_related=True,\n",
        "        top_k=5\n",
        "    )\n",
        "\n",
        "    if results_dict and query in results_dict:\n",
        "        results = results_dict[query]\n",
        "        related_queries = [q for q in results_dict.keys() if q != query]\n",
        "        \n",
        "        print(f\"\\n‚úÖ Found {len(results)} results for '{query}':\")\n",
        "        for i, result in enumerate(results, 1):\n",
        "            score_emoji = \"üü¢\" if result['similarity_score'] > 0.7 else \"üü°\" if result['similarity_score'] > 0.5 else \"üü†\"\n",
        "            print(f\"\\n{i}. {score_emoji} {result['time_formatted']} ({result['similarity_score']:.0%})\")\n",
        "            print(f\"   {result['caption']}\")\n",
        "            # Show the image for the first (top-1) result to avoid heavy output\n",
        "            if i == 1:\n",
        "                print(\"   üñºÔ∏è Frame preview:\")\n",
        "                try:\n",
        "                    display_frame_image(result.get('video_name', ''), float(result.get('timestamp', 0.0)))\n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ö†Ô∏è Preview error: {e}\")\n",
        "        \n",
        "        if related_queries:\n",
        "            print(f\"\\n   üîÑ Also found: {', '.join(related_queries[:3])} at similar times\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå No results found. Try a different query.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b8e85f723ff498cb32ecabbc9e8d7e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23620e7e22174d64a296a93987c17fec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ecf964c16ff4a08878fe34558a95210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41eb84dbfbaa4f44b86bc3c19e7b5ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62e897ad4342444da1192278dfee423e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef51046f4b9d4f69acbb93c2c06782d6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0c45d6c2582411da54de81095263e6c",
            "value": 1
          }
        },
        "842f35e1b1204c93878d78267040688e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9518db97018c4e9b8f600c1c505b8e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b8e85f723ff498cb32ecabbc9e8d7e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2ecf964c16ff4a08878fe34558a95210",
            "value": "‚Äá1/1‚Äá[00:00&lt;00:00,‚Äá23.69it/s]"
          }
        },
        "ba691f1082b9431788be4e50e8141b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c50b623424624bcabf621afcd01a7e1e",
              "IPY_MODEL_62e897ad4342444da1192278dfee423e",
              "IPY_MODEL_9518db97018c4e9b8f600c1c505b8e50"
            ],
            "layout": "IPY_MODEL_842f35e1b1204c93878d78267040688e"
          }
        },
        "c50b623424624bcabf621afcd01a7e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23620e7e22174d64a296a93987c17fec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_41eb84dbfbaa4f44b86bc3c19e7b5ae1",
            "value": "Batches:‚Äá100%"
          }
        },
        "ef51046f4b9d4f69acbb93c2c06782d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c45d6c2582411da54de81095263e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
